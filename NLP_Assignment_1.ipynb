{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NargesSamaeii/NLP_Assignment/blob/main/NLP_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baZ2eJiJ3A0Y",
        "outputId": "b2cf0bca-24e6-4c94-b451-33076678ebde"
      },
      "id": "baZ2eJiJ3A0Y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import wikipediaapi\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, FreqDist, download\n",
        "import numpy as np\n",
        "import requests\n"
      ],
      "metadata": {
        "id": "mMbycMao4YtU"
      },
      "id": "mMbycMao4YtU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZsWZch84atM",
        "outputId": "4dee32dd-8b7e-48dd-b51c-0afd65d227c4"
      },
      "id": "zZsWZch84atM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Wikipedia API\n",
        "wiki_wiki = wikipediaapi.Wikipedia('english', extract_format=wikipediaapi.ExtractFormat.WIKI, headers={'User-Agent': 'Narges'})\n",
        "\n",
        "# Function to retrieve text from a Wikipedia page\n",
        "def fetch_wikipedia_text(page_title):\n",
        "    try:\n",
        "        page = wiki_wiki.page(page_title)\n",
        "\n",
        "        if not page.exists():\n",
        "            print(f\"Wikipedia page '{page_title}' does not exist.\")\n",
        "            return None\n",
        "\n",
        "        return page.text\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while fetching Wikipedia page '{page_title}': {e}\")\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "NYXlyLk05HGs"
      },
      "id": "NYXlyLk05HGs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample annotated keywords for geographical and non-geographical topics\n",
        "geo_topics = ['Europe', 'Asia', 'Africa', 'North America', 'South America', 'Australia', 'Antarctica', 'Mountain', 'River', 'Desert']\n",
        "non_geo_topics = ['programming', 'technology', 'history', 'medical', 'estimates', 'behave', 'physic', 'economy']\n",
        "\n",
        "# Function to preprocess text with optional stemming and lemmatization\n",
        "def preprocess_text(text, stop_words, use_stemming=False, use_lemmatization=False):\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    if use_stemming:\n",
        "        stemmer = nltk.stem.SnowballStemmer('english')\n",
        "        words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    if use_lemmatization:\n",
        "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "        words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "\n",
        "# Function to extract keywords with optional stemming and lemmatization\n",
        "def extract_keywords(text, stop_words, use_stemming=False, use_lemmatization=False):\n",
        "    return preprocess_text(text, stop_words, use_stemming, use_lemmatization)\n",
        "\n",
        "# Function to extract nouns with optional stemming and lemmatization\n",
        "def extract_nouns(text, stop_words, use_stemming=False, use_lemmatization=False):\n",
        "    words = word_tokenize(text)\n",
        "    tagged_words = pos_tag(words)\n",
        "\n",
        "    if use_stemming:\n",
        "        stemmer = nltk.stem.SnowballStemmer('english')\n",
        "        tagged_words = [(stemmer.stem(word), pos) for word, pos in tagged_words]\n",
        "\n",
        "    if use_lemmatization:\n",
        "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "        tagged_words = [(lemmatizer.lemmatize(word), pos) for word, pos in tagged_words]\n",
        "\n",
        "    return [word.lower() for word, pos in tagged_words if pos.startswith('N') and word.lower() not in stop_words and word.isalnum()]\n",
        "\n",
        "# Function to extract top nouns from topics with optional stemming and lemmatization\n",
        "def find_top_nouns(topics, stop_words, num_top_nouns=10, use_stemming=False, use_lemmatization=False):\n",
        "    all_nouns = []\n",
        "\n",
        "    for topic in topics:\n",
        "        text = fetch_wikipedia_text(topic)\n",
        "        if text:\n",
        "            all_nouns.extend(extract_nouns(text, stop_words, use_stemming, use_lemmatization))\n",
        "\n",
        "    nouns_freq_dist = FreqDist(all_nouns)\n",
        "    return [word for word, _ in nouns_freq_dist.most_common(num_top_nouns)]\n"
      ],
      "metadata": {
        "id": "GSg2myVl5NuO"
      },
      "id": "GSg2myVl5NuO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f68c545-9dc5-4470-807a-50c04d94b32a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f68c545-9dc5-4470-807a-50c04d94b32a",
        "outputId": "7a342b4b-d235-4ea1-e70f-7bae2a4ae562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Customized Logistic Regression): 0.5\n",
            "Accuracy (Customized Na誰ve Bayes): 1.0\n",
            "Precision (Customized Na誰ve Bayes): 1.0\n",
            "Recall (Customized Na誰ve Bayes): 1.0\n",
            "The content of the \"Germany\" is geographical according to Customized Na誰ve Bayes with TF-IDF.\n",
            "The content of the \"Germany\" is non-geographical according to Customized Logistic Regression with TF-IDF.\n"
          ]
        }
      ],
      "source": [
        "# Extract top nouns for both geographical and non-geographical topics\n",
        "stop_words = set(stopwords.words('english'))\n",
        "top_geo_keywords = find_top_nouns(geo_topics, stop_words, num_top_nouns=10, use_stemming=True, use_lemmatization=True)\n",
        "top_non_geo_keywords = find_top_nouns(non_geo_topics, stop_words, num_top_nouns=10, use_stemming=True, use_lemmatization=True)\n",
        "\n",
        "all_topics = geo_topics + non_geo_topics\n",
        "all_docs = []\n",
        "all_labels = []\n",
        "\n",
        "for topic in all_topics:\n",
        "    text = fetch_wikipedia_text(topic)\n",
        "    if text:\n",
        "        keywords = extract_keywords(text, stop_words, use_stemming=True, use_lemmatization=True)\n",
        "        all_docs.append(\" \".join(keywords))\n",
        "        all_labels.append(1 if topic in geo_topics else 0)\n",
        "\n",
        "all_top_keywords = top_geo_keywords + top_non_geo_keywords\n",
        "vectorizer = TfidfVectorizer(vocabulary=all_top_keywords)\n",
        "X = vectorizer.fit_transform(all_docs)\n",
        "y = all_labels\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression classifier\n",
        "logistic_classifier = LogisticRegression()\n",
        "logistic_classifier.fit(X_train, y_train)\n",
        "logistic_predictions = logistic_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy for Logistic Regression\n",
        "accuracy_logistic = accuracy_score(y_test, logistic_predictions)\n",
        "print(f\"Accuracy (Customized Logistic Regression): {accuracy_logistic}\")\n",
        "\n",
        "subject = \"Germany\"\n",
        "website_text = fetch_wikipedia_text(subject)\n",
        "\n",
        "# Naive Bayes classifier\n",
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(X_train, y_train)\n",
        "naive_bayes_predictions = naive_bayes_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy, precision, and recall for Naive Bayes\n",
        "accuracy_naive_bayes = accuracy_score(y_test, naive_bayes_predictions)\n",
        "precision_naive_bayes = precision_score(y_test, naive_bayes_predictions)\n",
        "recall_naive_bayes = recall_score(y_test, naive_bayes_predictions)\n",
        "\n",
        "# Print the results for Naive Bayes\n",
        "print(f\"Accuracy (Customized Na誰ve Bayes): {accuracy_naive_bayes}\")\n",
        "print(f\"Precision (Customized Na誰ve Bayes): {precision_naive_bayes}\")\n",
        "print(f\"Recall (Customized Na誰ve Bayes): {recall_naive_bayes}\")\n",
        "\n",
        "if website_text:\n",
        "    # Vectorize the website text using TF-IDF\n",
        "    website_vectorized = vectorizer.transform([\" \".join(extract_keywords(website_text, stop_words, use_stemming=True, use_lemmatization=True))])\n",
        "\n",
        "    # Naive Bayes and Logistic Regression predictions using TF-IDF for the website\n",
        "    naive_bayes_website_prediction = naive_bayes_classifier.predict(website_vectorized)\n",
        "    logistic_website_prediction = logistic_classifier.predict(website_vectorized)\n",
        "\n",
        "    print(f'The content of the \"{subject}\" is geographical according to Customized Na誰ve Bayes with TF-IDF.' if naive_bayes_website_prediction == 1 else f'The content of the \"{subject}\" is non-geographical according to Customized Na誰ve Bayes with TF-IDF.')\n",
        "    print(f'The content of the \"{subject}\" is geographical according to Customized Logistic Regression with TF-IDF.' if logistic_website_prediction == 1 else f'The content of the \"{subject}\" is non-geographical according to Customized Logistic Regression with TF-IDF.')\n",
        "else:\n",
        "    print(f'Unable to fetch text from the \"{subject}\". Please check the Subject or try another subject.')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}