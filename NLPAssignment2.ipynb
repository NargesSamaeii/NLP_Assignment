{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NargesSamaeii/NLP_Assignment/blob/main/NLPAssignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d00f5280-3444-483a-9a5e-343e1554cb2c",
      "metadata": {
        "id": "d00f5280-3444-483a-9a5e-343e1554cb2c"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def read_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def tokenize_sentences(text):\n",
        "    return sent_tokenize(text)\n",
        "\n",
        "def tokenize_words(sentence):\n",
        "    return word_tokenize(sentence)\n",
        "\n",
        "def sentence_similarity(sent1, sent2, stopwords):\n",
        "    words1 = [word.lower() for word in sent1 if word.isalnum() and word.lower() not in stopwords]\n",
        "    words2 = [word.lower() for word in sent2 if word.isalnum() and word.lower() not in stopwords]\n",
        "\n",
        "    all_words = list(set(words1 + words2))\n",
        "\n",
        "    vector1 = [1 if word in words1 else 0 for word in all_words]\n",
        "    vector2 = [1 if word in words2 else 0 for word in all_words]\n",
        "\n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "def build_similarity_matrix(sentences, stopwords):\n",
        "    matrix = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        for j in range(len(sentences)):\n",
        "            if i != j:\n",
        "                matrix[i][j] = sentence_similarity(sentences[i], sentences[j], stopwords)\n",
        "\n",
        "    return matrix\n",
        "\n",
        "def generate_summary(document_text, context_window_size, style_text=None):\n",
        "    sentences = tokenize_sentences(document_text)\n",
        "    stopwords_list = set(stopwords.words('english'))\n",
        "\n",
        "    # Measure the length of the document\n",
        "    document_length = len(tokenize_words(document_text))\n",
        "\n",
        "    # Compute target lengths in a proportional way\n",
        "    target_lengths = [int(len(sentence) * (len(sentence) / document_length)) for sentence in sentences]\n",
        "\n",
        "    # Slice the document and generate summaries\n",
        "    summary = \"\"\n",
        "    for i in range(len(sentences)):\n",
        "        slice_start = 0 if i == 0 else int(sum(target_lengths[:i]))\n",
        "        slice_end = int(sum(target_lengths[:i + 1]))\n",
        "        slice_end = min(slice_end, len(sentences))  # Ensure not to go beyond the document length\n",
        "\n",
        "        # Slice the document\n",
        "        sliced_document = ' '.join(sentences[slice_start:slice_end])\n",
        "\n",
        "        # Summarize the slice\n",
        "        slice_summary = extractive_summarization(sliced_document, stopwords_list)\n",
        "\n",
        "        # Collate the summaries\n",
        "        summary += slice_summary\n",
        "\n",
        "    # Repeat shrinking activities until the summary size is within the context window\n",
        "    while len(tokenize_words(summary)) > context_window_size:\n",
        "        summary = extractive_summarization(summary, stopwords_list)\n",
        "\n",
        "    # Save the document (you can save the summary to a file if needed)\n",
        "    with open('summary.txt', 'w', encoding='utf-8') as file:\n",
        "        file.write(summary)\n",
        "\n",
        "    # Repeat the summarization for the second document (if needed)\n",
        "    if style_text:\n",
        "        style_summary = extractive_summarization(style_text, stopwords_list)\n",
        "        summary += style_summary\n",
        "\n",
        "    # Generate the query\n",
        "    query = generate_query(summary)\n",
        "\n",
        "    return summary, query\n",
        "\n",
        "def extractive_summarization(text, stopwords_list):\n",
        "    sentences = tokenize_sentences(text)\n",
        "    sentence_matrix = build_similarity_matrix(sentences, stopwords_list)\n",
        "\n",
        "    # Rank sentences based on similarity matrix\n",
        "    sentence_ranks = np.sum(sentence_matrix, axis=1)\n",
        "\n",
        "    # Sort sentences by rank\n",
        "    ranked_sentences = [sentences[i] for i in np.argsort(sentence_ranks)[::-1]]\n",
        "\n",
        "    # Select top sentences for the summary (you can adjust the summary length as needed)\n",
        "    summary_length = int(len(sentences) * 0.3)\n",
        "    summary = ' '.join(ranked_sentences[:summary_length])\n",
        "\n",
        "    return summary\n",
        "\n",
        "def generate_query(summary):\n",
        "    # Placeholder for query generation logic\n",
        "    query = \"Please provide relevant information about:\\n\" + summary\n",
        "    return query\n",
        "\n",
        "# Example usage:\n",
        "document_path = 'path_to_your_document.txt'  # Replace with the actual path to the document\n",
        "context_window_size = 128  # You can adjust this based on your requirements\n",
        "document_text = read_text(document_path)\n",
        "\n",
        "# Optional: Style text (provide another text for style transfer, if needed)\n",
        "style_text = 'path_to_your_style_text.txt'  # Replace with the actual path to the style text file\n",
        "\n",
        "# Generate summary and query\n",
        "generated_summary, generated_query = generate_summary(document_text, context_window_size, style_text)\n",
        "print(\"Generated Summary:\\n\", generated_summary)\n",
        "print(\"\\nGenerated Query:\\n\", generated_query)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}