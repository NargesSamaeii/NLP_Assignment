{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NargesSamaeii/NLP_Assignment/blob/main/NLP_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "36bWdQnXiQDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oohalNelC7P",
        "outputId": "573ee684-334e-4368-b590-88dc0996289d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_and_tokenize_text(file_path):\n",
        "    \"\"\"Reads text from a file, tokenizes it into sentences and words.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = [word_tokenize(sentence) for sentence in sentences]\n",
        "    return text, sentences, words\n",
        "\n",
        "def calculate_sentence_similarity(sent1, sent2, stopwords):\n",
        "    \"\"\"Calculates cosine similarity between two sentences.\"\"\"\n",
        "    words1 = [word.lower() for word in sent1 if word.isalnum() and word.lower() not in stopwords]\n",
        "    words2 = [word.lower() for word in sent2 if word.isalnum() and word.lower() not in stopwords]\n",
        "\n",
        "    all_words = list(set(words1 + words2))\n",
        "\n",
        "    vector1 = [1 if word in words1 else 0 for word in all_words]\n",
        "    vector2 = [1 if word in words2 else 0 for word in all_words]\n",
        "\n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "def build_similarity_matrix(sentences, stopwords):\n",
        "    \"\"\"Builds a similarity matrix for a list of sentences.\"\"\"\n",
        "    matrix_size = len(sentences)\n",
        "    matrix = np.zeros((matrix_size, matrix_size))\n",
        "\n",
        "    for i in range(matrix_size):\n",
        "        for j in range(matrix_size):\n",
        "            if i != j:\n",
        "                matrix[i][j] = calculate_sentence_similarity(sentences[i], sentences[j], stopwords)\n",
        "\n",
        "    return matrix\n",
        "\n",
        "def slice_and_summarize_document(sentences, target_lengths, stopwords_list, context_window_size):\n",
        "    \"\"\"Slices and summarizes a document based on target lengths.\"\"\"\n",
        "    summary = \"\"\n",
        "    included_sentences = set()\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        slice_start = 0 if i == 0 else int(sum(target_lengths[:i]))\n",
        "        slice_end = min(int(sum(target_lengths[:i + 1])), len(sentences))\n",
        "\n",
        "        sliced_document = ' '.join(sentences[slice_start:slice_end])\n",
        "        slice_summary = extractive_summarization(sliced_document, stopwords_list)\n",
        "\n",
        "        for sent in slice_summary.split('\\n'):\n",
        "            if sent not in included_sentences:\n",
        "                summary += sent + '\\n'\n",
        "                included_sentences.add(sent)\n",
        "\n",
        "    while len(word_tokenize(summary)) > context_window_size:\n",
        "        summary = extractive_summarization(summary, stopwords_list)\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "H6F2eZC6lEf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(document_text, context_window_size, style_text=None):\n",
        "    \"\"\"Generates a summary for a document with optional style text.\"\"\"\n",
        "    document_text, sentences, _ = read_and_tokenize_text(document_text)\n",
        "    stopwords_list = set(stopwords.words('english'))\n",
        "\n",
        "    document_length = len(word_tokenize(document_text))\n",
        "    target_lengths = [int(len(sentence) * (len(sentence) / document_length)) for sentence in sentences]\n",
        "\n",
        "    summary = slice_and_summarize_document(sentences, target_lengths, stopwords_list, context_window_size)\n",
        "\n",
        "    with open('summary.txt', 'w', encoding='utf-8') as file:\n",
        "        file.write(summary)\n",
        "\n",
        "    if style_text:\n",
        "        style_text, _, _ = read_and_tokenize_text(style_text)\n",
        "        style_summary = extractive_summarization(style_text, stopwords_list)\n",
        "        summary += style_summary\n",
        "\n",
        "    query = generate_query(summary)\n",
        "\n",
        "    return summary, query\n",
        "\n",
        "def extractive_summarization(text, stopwords_list):\n",
        "    \"\"\"Generates an extractive summary for a given text.\"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentence_matrix = build_similarity_matrix(sentences, stopwords_list)\n",
        "\n",
        "    sentence_ranks = np.sum(sentence_matrix, axis=1)\n",
        "    ranked_sentences = [sentences[i] for i in np.argsort(sentence_ranks)[::-1]]\n",
        "\n",
        "    summary_length = int(len(sentences) * 0.3)\n",
        "    summary = '\\n'.join(ranked_sentences[:summary_length])\n",
        "\n",
        "    return summary\n",
        "\n",
        "def generate_query(summary):\n",
        "    \"\"\"Generates a query based on the provided summary.\"\"\"\n",
        "    query = \"Please provide relevant information about:\\n\" + summary\n",
        "    return query"
      ],
      "metadata": {
        "id": "LNN0Vvw0lH5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "document_path = 'example.txt'\n",
        "context_window_size = 4000\n",
        "style_text_path = 'example_style_text.txt' if os.path.exists('example_style_text.txt') else None\n",
        "\n",
        "generated_summary, generated_query = generate_summary(document_path, context_window_size, style_text_path)\n",
        "print(\"Generated Summary:\\n\", generated_summary)\n",
        "print(\"\\nGenerated Query:\\n\", generated_query)"
      ],
      "metadata": {
        "id": "blJAKauIlFpf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}